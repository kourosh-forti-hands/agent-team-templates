# Agent Team Template — Population Guide

> **What this is:** A prompt you give to Claude Code to kick off an interactive session that
> populates `AGENT_TEAM_TEMPLATE.md` into a concrete, project-specific execution guide.
>
> **How to use it:**
> 1. Complete the **Prerequisites** below
> 2. Place the required files in your project root
> 3. Open Claude Code in your project directory
> 4. Paste the **Kickoff Prompt** into Claude Code
> 5. Claude Code will run a structured interview across 7 rounds of questions
> 6. After the interview, it produces a populated execution guide + planning files
> 7. You review, adjust, and approve before any execution begins
>
> **Philosophy:** Ask everything. Assume nothing. Every question shapes the plan.
> The quality of the execution guide is directly proportional to the thoroughness of the interview.

---

## Prerequisites

### Files You Need

Download both of these files and place them in your **project root directory** (the same directory you open Claude Code from):

| File | What it is | Required |
|------|-----------|----------|
| `AGENT_TEAM_TEMPLATE.md` | The template that gets populated into your execution guide. Claude Code reads this during the interview. **Do not edit this file** — it's the source template. | Yes |
| `AGENT_TEAM_POPULATE_GUIDE.md` | This file. Contains the kickoff prompt and reference docs. | Yes (contains the prompt) |

```
your-project/
├── AGENT_TEAM_TEMPLATE.md          ← download
├── AGENT_TEAM_POPULATE_GUIDE.md    ← download (this file)
├── src/                            ← your existing code (if any)
└── ...
```

### Claude Code Setup

You need [Claude Code](https://docs.anthropic.com/en/docs/claude-code) (Anthropic's CLI) installed and authenticated.

```bash
# Install Claude Code if you haven't
npm install -g @anthropic-ai/claude-code

# Verify it's working
claude --version
```

**Model access:** The template references Opus, Sonnet, and Haiku models for different teammate roles. You need a Claude Code plan that supports the models you intend to use. Sonnet is the default for teammates; Opus for the lead/complex phases.

### Agent Teams Feature

The squad and swarm topologies use Claude Code's agent team features (`TeamCreate`, `TaskCreate`, `SendMessage`, etc.). These are built into Claude Code — no extra setup needed.

If you only plan to use the **solo** topology (no teams, sequential execution), you can skip this. Solo mode only uses the basic `Task()` tool which works in all Claude Code versions.

### MCP Servers (Optional — Enhances Capabilities)

The template references several MCP servers for research, code analysis, and memory. **None are strictly required** — the interview and execution will adapt based on what's available. But each one you add expands what the populated guide can do.

| MCP Server | What it provides | Used for |
|------------|-----------------|----------|
| **Memory** (`mcp__memory__*`) | Persistent key-value knowledge graph | Storing Decision_ entities across phases and sessions. Cross-teammate state sharing. |
| **Serena** (`mcp__serena__*`) | Semantic code analysis | Symbol-level code reading/editing, codebase overview, pattern search |
| **Brave Search** (`mcp__brave-search__*`) | Web search | Technology research in Phase 1, finding docs/examples |
| **PAL** (`mcp__pal__*`) | Multi-model AI analysis | Deep thinking, code review, consensus, API lookup |
| **Octagon** (`mcp__octagon-deep-research-mcp__*`) | Deep research agent | Extended research for complex technology decisions |
| **Fetch** (`mcp__fetch__*`) | HTTP requests | Testing API endpoints, health checks |

**To configure MCP servers**, use `claude mcp add` or edit `.claude/settings.json` in your project (or `~/.claude/settings.json` globally). Each MCP server has its own installation instructions — check the server's documentation for the correct `claude mcp add` command or npm package name.

```bash
# Example: add Memory MCP server
claude mcp add memory npx -y @modelcontextprotocol/server-memory
```

**If you have NONE of these:** The template still works. The interview will populate fields the same way. During execution, phases that reference unavailable MCP tools will use Claude Code's built-in tools (Grep, Glob, Read, WebSearch, etc.) as fallbacks. Research steps will use `WebSearch` instead of Brave/Octagon. Code analysis will use Grep/Read instead of Serena. You lose cross-session Memory persistence but planning files (task_plan.md, findings.md, progress.md) still carry state.

### Claude Code Skills/Plugins (Optional — Referenced by the Template)

The template references these Claude Code skills. They are **optional** — if unavailable, Claude Code will do the equivalent work manually.

| Skill | What it does | Referenced in |
|-------|-------------|---------------|
| `/planning-with-files:plan` | Creates task_plan.md, findings.md, progress.md | Step 0 Bootstrap, After Interview |
| `/checkpoint:create` | Saves a named recovery point | End of every phase |
| `/code-review` | Runs code review on changed files | Phase F |
| `/validate-and-fix` | Auto-fixes linting/formatting issues | Phase F |

If you don't have these skills installed, the template's instructions still make sense — Claude Code will create the planning files manually and skip the checkpoint/review commands. You can always add them later.

### Quick Checklist Before Starting

- [ ] Claude Code installed and authenticated (`claude --version` works)
- [ ] `AGENT_TEAM_TEMPLATE.md` in your project root
- [ ] `AGENT_TEAM_POPULATE_GUIDE.md` in your project root (this file)
- [ ] (Optional) MCP servers configured for Memory, Serena, Brave, PAL, etc.
- [ ] (Optional) Planning-with-files skill installed
- [ ] (Optional) A prompt file or spec document describing your project (if you have one)
- [ ] Open Claude Code in your project root directory: `cd your-project && claude`

---

## Kickoff Prompt

Copy everything between the `---START---` and `---END---` markers and paste it into Claude Code.

```
---START---

You are going to help me create an Agent Team Execution Guide for a project.

Read the template file first:
- Read: AGENT_TEAM_TEMPLATE.md

Then conduct a structured interview to populate every field in the template.
Do NOT infer, assume, or fill in defaults on my behalf. ASK me.
Every question you ask shapes the final plan — thoroughness here prevents rework later.

## Interview Rules

1. **Ask, don't assume.** If you're unsure about something, ask. If you think you know but aren't certain, still ask. If a question seems obvious, ask anyway — my answer might surprise you.

2. **One round at a time.** Use AskUserQuestion for structured choices. Ask 1-4 questions per round. Wait for my answers before proceeding to the next round.

3. **Explain why you're asking.** Before each question, give a 1-2 sentence explanation of what this decision affects and why it matters. Help me make informed choices.

4. **Offer research as an option.** For every technical decision, include "I don't know yet — research this in Phase 1" as a valid answer. This maps to the `research:` field in the template.

5. **Build on prior answers.** Each round should incorporate context from my previous answers. If I said "Python backend" in Round 1, Round 3 shouldn't ask about Go frameworks.

6. **Challenge when appropriate.** If my answers seem contradictory or risky, say so. "You mentioned self-hosted with no DevOps experience, but chose Kubernetes — want to reconsider?" is helpful.

7. **Track decisions.** After each round, briefly summarize what was decided and what's still open. Keep a running tally.

## Interview Structure

Conduct the interview in these 7 rounds. Complete each round before moving to the next.

### Round 1: Project Identity & Vision
**Goal:** Understand WHAT we're building and WHY.

Ask about:
- Project name and slug (for file naming)
- Goal summary — what does "done" look like in 1-2 sentences?
- Is there an existing codebase, or is this greenfield?
- If existing: what's the repo structure? What works? What's broken?
- If greenfield: is there a design doc, spec, or prompt file that describes the project?
- Who are the users? How many? Where are they?
- What's the deployment target? (self-hosted, cloud, hybrid)
- Is there a prompt file this guide should execute? (e.g., PROMPT_SOMETHING.md)

### Round 2: Technology Stack
**Goal:** Pin down every technology choice OR mark it for research.

For EACH of these, ask separately (don't bundle):
- Backend language — or should Phase 1 research this?
- Backend framework — or research?
- Frontend framework — or research?
- Database — or research?
- Cache/pub-sub layer — or research? Or "none needed"?
- Deployment tooling (Docker Compose, K8s, serverless, etc.) — or research?
- Client type (web SPA, desktop app, mobile, CLI, etc.) — or research?

For each "research" answer, ask:
- What constraints should the research respect? (e.g., "must be Python", "must be self-hostable")
- Is there a shortlist to compare, or is it fully open?

### Round 3: Services & Architecture
**Goal:** Define the service boundaries and project structure.

Ask about:
- What services/modules exist (or should exist)?
- Or should Phase 1 research the service boundaries?
- Where do services live in the repo? (e.g., `backend/services/`, `src/`, `packages/`)
- What's the entry file per service? (e.g., `main.py`, `index.ts`, `app.go`)
- How do services communicate? (HTTP, gRPC, message queue, shared DB)
- Is there a shared library or common code?
- Are there any external dependencies or third-party APIs?

### Round 4: Features & Critical Path
**Goal:** Enumerate every feature that needs implementation and identify the riskiest one.

Ask about:
- List every feature that needs to be built or completed
- For EACH feature:
  - What category does it fall into? (e.g., "auth", "messaging", "storage")
  - Do you already know HOW to implement it, or should Phase 1 research the approach?
  - If known: what's the approach in one sentence?
  - If research: what constraints? What alternatives to compare?
  - Does it depend on other features?
- Which feature is the CRITICAL PATH — the hardest, riskiest, or most uncertain?
  - Why is it critical?
  - Should we use the "competing hypotheses" pattern (3 research agents investigate different approaches, then a challenger picks the winner)?
- Are there features that are INDEPENDENT and can run in parallel?

### Round 5: Team Topology & Execution Strategy
**Goal:** Decide how the work gets organized and parallelized.

Present the three topologies with their trade-offs:
- **Solo**: No teams. Lead runs phases sequentially via Task() calls. Cheapest, simplest, easiest to review. Best for small projects or tightly coupled features.
- **Squad**: One persistent team. All tasks created upfront with dependencies. Teammates spawned as tasks become unblocked. Good balance of parallelism and simplicity.
- **Swarm**: Per-wave teams. Maximum parallelism and isolation. Teams created/destroyed per wave. Highest throughput but highest cost. Best for large projects with clear boundaries.

Ask about:
- Which topology fits this project?
- If squad/swarm:
  - Should the lead be delegate-only (coordinates, never codes) or hands-on?
  - Should teammates require plan approval before implementing?
  - What model should teammates default to? (sonnet = balanced, opus = highest quality, haiku = fastest/cheapest)
- How should phases group into waves? Walk through the features and ask:
  - Which phases MUST be sequential (depends on prior phase)?
  - Which phases CAN run in parallel (independent of each other)?
  - Draw out the wave structure together
- For parallel waves: who owns which files? (Two teammates must never edit the same file)

### Round 6: Success Criteria & Quality Gates
**Goal:** Define exactly what "done" means — testable, verifiable criteria.

Ask about:
- Functional criteria: What must the system DO? (list specific behaviors)
  - Or should Phase 1 research what "good" looks like for this type of project?
- Infrastructure criteria: What must be in place? (SSL, backups, monitoring, etc.)
- Deployment criteria: What makes deployment "done"? (single command? docs? CI/CD?)
- Performance criteria: What numbers must be hit? (latency, throughput, scale)
  - Or should Phase 1 research reasonable benchmarks?
- Quality gates for teammates:
  - Must tests pass before a task is marked complete?
  - Must code be reviewed (by lead or peer)?
  - Must a Memory checkpoint be saved?

### Round 7: Phase Design & Customization
**Goal:** Flesh out the specific phases beyond the template defaults.

Ask about:
- Phase 2 (Foundation): What does the foundation phase build? Core APIs? Auth? Data layer? All of the above?
- For each feature phase (from Round 4): any specific implementation details, constraints, or instructions that should be embedded in the teammate prompt?
- Client integration phase: Is there a separate client? If so, what needs wiring up?
- Deployment phase: Any specific deployment requirements beyond the template defaults? (custom domains, specific cloud provider setup, compliance requirements)
- Testing phase: Any domain-specific tests beyond the standard unit/integration/E2E/load/security?
- Are there any phases the template doesn't cover that this project needs? (e.g., data migration, content seeding, third-party integration setup)

## After the Interview

Once all 7 rounds are complete:

1. **Summarize all decisions** in a structured format, showing:
   - Concrete values (what's decided)
   - Research questions (what Phase 1 will resolve)
   - Wave structure (what runs when and in parallel with what)

2. **Ask me to confirm** the full summary before generating anything.

3. **Initialize planning files:**
   - Run: /planning-with-files:plan '{{PROJECT_NAME}} - Execution Guide Population'
   - This creates task_plan.md, findings.md, progress.md
   - Write the interview decisions into findings.md as the initial knowledge base
   - Write the phase structure into task_plan.md as the execution plan

4. **Generate the populated execution guide:**
   - Read AGENT_TEAM_TEMPLATE.md
   - Replace ALL {{PLACEHOLDER}} values with concrete answers or research requests from the interview
   - Remove template instructions (the "> Template note:" blocks)
   - Remove unused optional sections (e.g., Phase Y if no client)
   - Add any custom phases identified in Round 7
   - Compute: dependency graph, wave mapping, file ownership matrix
   - Write the result to: AGENT_TEAM_EXECUTION_GUIDE.md

5. **Generate the Input Criteria YAML block** separately:
   - Write to: agent_team_input_criteria.yaml
   - This is the filled-in YAML from the template's Input Criteria section
   - Useful as a machine-readable record of all decisions

6. **Update planning files with the execution plan:**
   - task_plan.md: List every phase as a task with status: pending
   - findings.md: All interview decisions + any research questions flagged
   - progress.md: Record that the interview is complete, guide is generated

7. **Present the final execution guide** for my review.
   - Walk me through the wave structure
   - Highlight any phases where you had to make a judgment call
   - Ask if anything needs adjustment before execution begins

## Important Notes

- The populated guide should use `/planning-with-files:plan` in Step 0 Bootstrap to re-initialize planning files for the actual execution (separate from the planning files used during this interview)
- Every phase in the populated guide should read and write to task_plan.md, findings.md, and progress.md
- The planning files are the GROUND TRUTH for project state — they persist across context windows, sessions, and teammate boundaries
- Memory MCP entities supplement the planning files for cross-teammate state sharing
- Both systems (planning files + Memory) should be kept in sync

---END---
```

---

## What the Interview Produces

After the 7-round interview, Claude Code generates these artifacts:

```
Project Root/
├── AGENT_TEAM_EXECUTION_GUIDE.md     # The populated execution guide (main artifact)
├── agent_team_input_criteria.yaml     # Machine-readable record of all decisions
├── task_plan.md                       # Phase breakdown with status tracking
├── findings.md                        # Interview decisions + research questions
└── progress.md                        # Interview completion record
```

### Artifact Relationships

```
┌──────────────────────────────────────────────────────────────────────────┐
│                        INTERVIEW (7 rounds)                              │
│                                                                          │
│  Round 1: Identity ──→ Round 2: Tech ──→ Round 3: Architecture           │
│  Round 4: Features ──→ Round 5: Topology ──→ Round 6: Criteria           │
│  Round 7: Phase Details                                                  │
│                                                                          │
│  Every answer feeds into ──┐                                             │
└────────────────────────────┼─────────────────────────────────────────────┘
                             │
                             ▼
              ┌──────────────────────────────┐
              │  AGENT_TEAM_TEMPLATE.md      │ (read-only template)
              │  + Interview Answers         │
              │  = Populated Execution Guide │
              └──────────────┬───────────────┘
                             │
              ┌──────────────┴──────────────────────┐
              │                                     │
              ▼                                     ▼
   ┌──────────────────────┐          ┌───────────────────────────┐
   │ Planning files:       │          │ AGENT_TEAM_EXECUTION_     │
   │  task_plan.md         │          │ GUIDE.md                  │
   │  findings.md          │          │ + agent_team_input_       │
   │  progress.md          │          │   criteria.yaml           │
   └──────────────────────┘          └───────────────────────────┘
```

### How Planning Files Flow Through Execution

```
INTERVIEW PHASE                           EXECUTION PHASE
═══════════════                           ═══════════════

findings.md ◄── Interview decisions       findings.md ◄── Gap analysis, research results,
                                                          security findings, teammate discoveries

task_plan.md ◄── Phase list (all pending) task_plan.md ◄── Phase status updated per-phase
                                                           (pending → in_progress → complete)

progress.md ◄── "Interview complete,      progress.md ◄── Per-phase action log, test results,
                 guide generated"                          teammate progress, checkpoint records
```

**Important:** Step 0 Bootstrap in the execution guide re-initializes the planning files
for the actual build. The interview-phase planning files serve as input/reference only.

---

## Example: What a Populated Round Looks Like

Here's what Round 4 might look like in practice:

```
Claude Code:  Round 4 is about features and critical path. I need to understand
              every feature that needs building, which ones are risky, and which
              can run in parallel.

              Let's start with the feature list.

[AskUserQuestion]
  Question: "What features need to be built or completed for this project?"
  Header: "Features"
  Options:
    - "I have a specific list I'll describe"
    - "There's a spec/prompt file that lists them"
    - "I need help figuring out what features are needed"
    - (Other)

User: "I have a specific list I'll describe"

Claude Code:  Go ahead — list each feature with a short description.
              I'll ask follow-up questions about each one.

User: "1. User auth with JWT
       2. Real-time messaging via WebSocket
       3. Voice channels (WebRTC)
       4. File uploads with previews
       5. Server/channel management"

Claude Code:  Five features. Let me ask about each one.

[AskUserQuestion]
  Question: "For User Auth (JWT) — do you already know the implementation
            approach, or should Phase 1 research the best auth pattern?"
  Header: "Auth approach"
  Options:
    - "I know: JWT with refresh tokens, bcrypt passwords"
    - "Research: Compare JWT vs session-based vs OAuth for this use case"
    - "I have a rough idea but want validation"
    - (Other)

... (continues for each feature) ...

Claude Code:  Which of these 5 features is the CRITICAL PATH — the one
              most likely to fail, take longest, or block everything else?

              This matters because for critical-path features, we can use
              the "competing hypotheses" pattern: spawn 3 research agents
              to investigate different approaches simultaneously, then a
              challenger picks the winner before implementation begins.

[AskUserQuestion]
  Question: "Which feature is the critical path (riskiest/hardest)?"
  Header: "Critical path"
  Options:
    - "Voice channels (WebRTC) — most technically complex"
    - "Real-time messaging — core to everything else"
    - "None stands out — they're roughly equal"
    - (Other)
```

---

## Customizing the Interview

### Adding Rounds

If your project has domain-specific concerns not covered by the 7 default rounds, add them to the kickoff prompt:

```
### Round 8: [Your Domain]
**Goal:** [What this round determines]

Ask about:
- [Question 1]
- [Question 2]
- ...
```

### Skipping Rounds

If you already have a fully specified project (all concrete values, no research needed), you can skip rounds by pre-filling answers in the kickoff prompt:

```
## Pre-filled Answers (skip these questions)

- Project name: "Voice Chat"
- Tech stack: all concrete (FastAPI, Python, React, PostgreSQL, Redis, Docker Compose, Electron)
- Services: auth, user, server, channel, message, file, webrtc, websocket
- Team topology: solo (no teams)

## Start the interview at Round 4 (Features)
```

### Deepening Rounds

For complex projects, you can request deeper investigation in specific rounds:

```
## Deep Dive Requests

- Round 2 (Tech Stack): For every research field, I want you to do preliminary
  web research DURING the interview (not just in Phase 1) so I can make a more
  informed choice. Use Brave search + PAL thinkdeep before presenting options.

- Round 5 (Topology): Show me a concrete token cost estimate for each topology
  based on my feature count and wave structure.
```

---

## Quick Reference: Interview → Template Field Mapping

| Interview Round | Template Section | Key Fields Populated |
|-----------------|-----------------|---------------------|
| Round 1: Identity | PROJECT IDENTITY | project_name, project_slug, prompt_file, goal_summary |
| Round 1: Identity | DEPLOYMENT TARGET | hosting, target_users, geo_distribution |
| Round 2: Tech Stack | ARCHITECTURE | tech_stack.* (each as value or research) |
| Round 3: Architecture | ARCHITECTURE | services, service_base_path, service_entry_file |
| Round 4: Features | FEATURES | features[] (name, category, critical_path, approach) |
| Round 5: Topology | TEAM TOPOLOGY | team_topology, team_config.* (waves, teammates) |
| Round 6: Criteria | SUCCESS CRITERIA | success_criteria.* (functional, infra, deploy, perf) |
| Round 6: Criteria | TEAM TOPOLOGY | quality_gates.* |
| Round 7: Phase Design | Phase sections | Per-phase customization, custom phases |
