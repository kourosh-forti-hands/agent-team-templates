# Ralph Loop Template — Population Guide

> **What this is:** A prompt you give to Claude Code to kick off an interactive session that
> populates `RALPH_LOOP_TEMPLATE.md` into a concrete, project-specific execution guide.
>
> **How to use it:**
> 1. Complete the **Prerequisites** below
> 2. Place the required files in your project root
> 3. Open Claude Code in your project directory
> 4. Paste the **Kickoff Prompt** into Claude Code
> 5. Claude Code will run a structured interview across 6 rounds of questions
> 6. After the interview, it produces a populated execution guide + planning files
> 7. You review, adjust, and approve before any execution begins
>
> **Philosophy:** Ask everything. Assume nothing. The loop model gives you fine-grained control
> over iteration counts, promise naming, and execution cadence — the interview ensures those
> controls are set correctly for your project.

---

## Prerequisites

### Files You Need

Download both of these files and place them in your **project root directory** (the same directory you open Claude Code from):

| File | What it is | Required |
|------|-----------|----------|
| `RALPH_LOOP_TEMPLATE.md` | The template that gets populated into your execution guide. Claude Code reads this during the interview. **Do not edit this file** — it's the source template. | Yes |
| `RALPH_LOOP_POPULATE_GUIDE.md` | This file. Contains the kickoff prompt and reference docs. | Yes (contains the prompt) |

```
your-project/
├── RALPH_LOOP_TEMPLATE.md          ← download
├── RALPH_LOOP_POPULATE_GUIDE.md    ← download (this file)
├── src/                            ← your existing code (if any)
└── ...
```

### Claude Code Setup

You need [Claude Code](https://docs.anthropic.com/en/docs/claude-code) (Anthropic's CLI) installed and authenticated.

```bash
# Install Claude Code if you haven't
npm install -g @anthropic-ai/claude-code

# Verify it's working
claude --version
```

**Model access:** The template references Opus, Sonnet, and Haiku for different purposes. Sonnet is the default for feature phases; Opus for assessment and critical-path phases.

### Ralph Loop Skill

The template uses the `ralph-loop` skill (`/ralph-loop:ralph-loop`) to drive iterative execution with promise-based completion detection. This skill needs to be installed in your Claude Code environment.

- `/ralph-loop:ralph-loop "prompt" --max-iterations N --completion-promise "PROMISE_NAME"` — starts a loop
- `/ralph-loop:cancel-ralph` — cancels a stalled loop
- `/ralph-loop:help` — explains the skill

If you don't have this skill installed, the interview will still produce a valid guide — but you'll need to either install the skill before execution, or manually run each phase's prompt as a regular Claude Code command (losing the automatic iteration counting and promise detection).

### Planning-with-Files Skill (Recommended)

The template uses `/planning-with-files:plan` to create three persistent planning files:
- `task_plan.md` — phases, status tracking, key decisions
- `findings.md` — research results, gap analysis, architecture notes
- `progress.md` — action log, test results, checkpoints

These files are the durable state layer that survives across context windows. If you don't have this skill, the interview will generate instructions to create these files manually.

### MCP Servers (Optional — Enhances Capabilities)

The template references several MCP servers for research, code analysis, and memory. **None are strictly required** — the interview and execution will adapt based on what's available.

| MCP Server | What it provides | Used for |
|------------|-----------------|----------|
| **Memory** (`mcp__memory__*`) | Persistent knowledge graph | Storing `Decision_` entities across phases and sessions. Resolved research → concrete values. |
| **Serena** (`mcp__serena__*`) | Semantic code analysis | Symbol-level code reading/editing, codebase overview, pattern search |
| **Brave Search** (`mcp__brave-search__*`) | Web search | Technology research in Phase 1, finding docs/examples |
| **PAL** (`mcp__pal__*`) | Multi-model AI analysis | Deep thinking, code review, consensus, API lookup |
| **Octagon** (`mcp__octagon-deep-research-mcp__*`) | Deep research agent | Extended research for complex technology decisions |
| **Fetch** (`mcp__fetch__*`) | HTTP requests | Testing API endpoints, health checks |

**To configure MCP servers**, use `claude mcp add` or edit `.claude/settings.json`. Each MCP server has its own installation instructions — check the server's documentation for the correct `claude mcp add` command or npm package name.

```bash
# Example: add Memory MCP server
claude mcp add memory npx -y @modelcontextprotocol/server-memory
```

**If you have NONE of these:** The template still works. Research steps will use `WebSearch` instead of Brave/Octagon. Code analysis will use Grep/Read instead of Serena. You lose cross-session Memory persistence but planning files still carry state between phases.

### Checkpoint Skill (Recommended)

The template uses `/checkpoint:create` to save git-based snapshots between phases. If you don't have this skill, replace checkpoint steps with manual `git commit` commands.

### Quick Checklist

Before pasting the kickoff prompt:

- [ ] `RALPH_LOOP_TEMPLATE.md` is in your project root
- [ ] Claude Code is installed and authenticated (`claude --version`)
- [ ] Ralph Loop skill is available (`/ralph-loop:help` should respond)
- [ ] (Optional) Planning-with-files skill is available
- [ ] (Optional) Memory MCP server is configured
- [ ] (Optional) Serena is configured and your project is onboarded
- [ ] (Optional) Brave Search, PAL, Octagon MCP servers are configured

---

## Kickoff Prompt

Copy everything between `---START---` and `---END---` and paste it into Claude Code.

---START---

You are about to run a structured interview to populate the `RALPH_LOOP_TEMPLATE.md` template for my project. The output will be a populated `RALPH_LOOP_EXECUTION_GUIDE.md` tailored to my specific project.

## Your Role

You are a technical project planner specializing in iterative, loop-driven execution plans. Your job is to ask thorough questions across 6 rounds, then produce a populated execution guide that uses the ralph-loop skill (`/ralph-loop:ralph-loop`) for iterative phase execution with promise-based completion detection.

## How the Interview Works

**6 rounds of questions.** Each round focuses on a specific topic area. You ask all questions for one round, wait for my answers, then proceed to the next round. Each round builds on the answers from previous rounds.

**Rules:**
1. **Ask, don't assume** — If there are multiple valid choices, present them with trade-offs and ask me to pick. Never silently fill in a field with your best guess.
2. **One round at a time** — Present all questions for a round in a single message. Wait for my answers before moving to the next round.
3. **Explain WHY you're asking** — For each question, briefly explain what this decision affects in the execution plan.
4. **Offer the `research:` option** — For any field where I might not know the answer, remind me that I can say "research this" and it becomes an open question for Phase 1 to resolve. Explain what gets deferred and what that costs (extra iterations in Phase 1).
5. **Build on prior answers** — Reference earlier decisions when asking new questions. "Since you chose FastAPI in Round 2, this affects how we structure the services..."
6. **Challenge contradictions** — If an answer conflicts with a prior one, point it out and ask me to resolve it.
7. **Track what's concrete vs. research** — Maintain a running count of concrete values vs. research questions. This directly determines Phase 1 complexity and the recommended execution mode.

## Round Structure

### Round 1: Project Identity & Existing State
**Purpose:** Understand what exists today and what we're building toward.
**What this affects:** Project naming, goal framing, whether Phase 1 is pure research vs. gap analysis of existing code.

Ask about:
- Project name and slug (for file naming, memory entities, team names)
- One-sentence goal summary (this appears at the top of every phase prompt)
- Is there an existing codebase? If yes, what's its state? (greenfield vs. existing affects every phase)
- Is there a prompt file / spec document that describes the project? (this becomes `{{PROMPT_FILE}}`)
- What's the current development environment? (local machine, cloud IDE, CI/CD?)

### Round 2: Technology Stack
**Purpose:** Fill in the `tech_stack` section. Each field is either concrete or becomes a research question.
**What this affects:** Every implementation phase reads these decisions. Research fields add iterations to Phase 1.

For each of these fields, ask me to provide a concrete value OR say "research this":
- Backend framework (and language)
- Frontend framework (if applicable — some projects are backend-only)
- Database
- Cache layer (or "none needed")
- Deployment target (Docker Compose, K8s, serverless, etc.)
- Client type (web, desktop, mobile, CLI — or "N/A" for backend-only)

For each field, provide 2-3 common options with brief trade-offs to help me decide.

Also ask:
- Are there any hard constraints on the tech stack? ("Must be Python", "No Kubernetes", etc.)
- Are there technologies you've already committed to that I should treat as fixed?

### Round 3: Services & Architecture
**Purpose:** Define the service boundaries and where code lives.
**What this affects:** Phase 2 iterates per-service. The service list determines how many implementation steps and how many potential parallel phases.

Ask about:
- Service list — either concrete (list them) or research ("what services make sense for...?")
- Where services live in the repo (`service_base_path`)
- Entry file per service (`service_entry_file`)
- How do services communicate? (HTTP, gRPC, message queue, shared DB?)
- Are there shared libraries or common code across services?
- Hosting target (self-hosted, cloud, hybrid)
- Target user scale (this calibrates deployment and performance phases)
- Geographic distribution (single server, multi-region, etc.)

### Round 4: Features & Critical Path
**Purpose:** Define what gets built. Each feature becomes its own implementation phase. Identifying the critical path feature is essential for iteration budget and execution order.
**What this affects:** Number of Phase N blocks, iteration budgets, promise naming, execution mode recommendation.

For each feature:
- Feature name and category slug
- Is this on the critical path? (hardest/riskiest feature — gets extra iteration budget and may use competing hypotheses pattern)
- Implementation approach — concrete value or research question?
- Any constraints on the approach?
- Dependencies on other features? (Feature B needs Feature A's API to exist)

Also ask:
- Are there features you're NOT building yet but plan to later? (helps me avoid painting you into a corner)
- For the critical path feature: do you want the competing hypotheses pattern? (multiple research agents investigate different approaches, then a challenger picks the winner)

### Round 5: Execution Mode & Loop Configuration
**Purpose:** Choose how phases execute and calibrate the loop parameters. This is unique to the ralph-loop model — it determines iteration budgets, promise naming, and whether teams are used.
**What this affects:** The `execution_mode` field, `team_config` section, and every `--max-iterations` and `--completion-promise` parameter.

Based on what I've learned so far, present my situation and recommend an execution mode:

**Execution modes explained:**
- **Sequential** — Each phase runs as its own `/ralph-loop:ralph-loop` command. You review output between phases. Cheapest, easiest to follow. Best when: few features, tight dependencies, or you want maximum control.
- **Team** — Phases grouped into waves. Independent phases in a wave run as parallel teammates, each running their own ralph-loop internally. The team lead orchestrates. Best when: many features with clear boundaries, want maximum throughput.
- **Hybrid** — Some waves run sequentially (dependent phases), others spawn parallel teammates. Best when: mix of dependent and independent phases.

Ask about:
- Does the recommended mode match your preference? Or do you want a different one?
- For team/hybrid mode:
  - Do you want the lead to delegate only (never write code) or also implement?
  - Should teammates get plan approval before coding?
  - Default model for teammates? (sonnet is cheapest, opus for complex phases)
  - Which phases can run in parallel? (I'll propose wave groupings based on feature dependencies)

Then ask about loop parameters:
- Are you comfortable with the suggested max iterations per phase? (I'll show a table based on the Max Iteration Guidelines)
- Do you want a conservative budget (lower iterations, may need re-runs) or generous (higher iterations, more tokens but fewer re-runs)?
- Phase 1 iteration budget is especially important — with {{N}} research fields, I recommend {{X}} iterations. Agree?

### Round 6: Success Criteria & Promise Design
**Purpose:** Define what "done" looks like — both for the project overall and for each phase individually. Promise names must be specific and unambiguous.
**What this affects:** The `success_criteria` YAML section, every `<promise>` tag, and Phase F's validation checklist.

Ask about success criteria in four categories:
- **Functional** — What must the system do? (specific, testable behaviors)
- **Infrastructure** — What must the infrastructure provide? (SSL, backups, monitoring, etc.)
- **Deployment** — What must the deployment experience be? (single command, documented .env, etc.)
- **Performance** — What must the system achieve? (latency, throughput, scale)

For each criterion, ask if they have a concrete threshold or want Phase 1 to research reasonable targets.

Then walk through promise naming:
- Each phase gets a `<promise>PROMISE_NAME</promise>` that signals completion
- The promise should be descriptive and unique (e.g., `MESSAGING_FEATURE_COMPLETE` not `PHASE3_DONE`)
- Done criteria for each promise must be specific — what exactly must be true?
- Present my proposed promise names and done criteria for review

## After the Interview

Once all 6 rounds are complete:

1. **Summarize all decisions** in a structured format, showing concrete values vs. research questions
2. **Ask me to confirm** the summary before generating anything
3. **Initialize planning files**: Run `/planning-with-files:plan '{{PROJECT_NAME}} - {{GOAL_SUMMARY}}'` to create task_plan.md, findings.md, progress.md
4. **Generate the populated guide**: Read `RALPH_LOOP_TEMPLATE.md` and produce `RALPH_LOOP_EXECUTION_GUIDE.md` with:
   - All `{{PLACEHOLDER}}` fields replaced with concrete values or `research:` entries
   - Input Criteria YAML fully populated
   - Phase sections populated (Step 0, Phase 1, Phase 2, Phase N copies, Phase Y if needed, Phase D, Phase F)
   - Promise names and done criteria filled in
   - Max iterations set per phase
   - Execution mode and team_config populated (if team/hybrid)
   - Wave definitions populated (if team/hybrid)
   - Dependency graph and wave mapping populated
   - File ownership matrix populated (if team mode)
   - Quick reference table at the end with all phases, promises, and iteration counts
5. **Update planning files**: Write the phase breakdown into task_plan.md, research questions into findings.md, execution plan into progress.md
6. **Present for review**: Show me the key sections of the generated guide and ask for final adjustments

## Important Notes

- The ralph-loop model gives YOU control between phases. After each loop completes, you review before starting the next one. The guide tells you exactly what command to paste next.
- Research questions cost Phase 1 iterations but give you better decisions. Don't be afraid to mark things as `research:` — that's what Phase 1 is for.
- Promise naming matters. A vague promise means a loop that doesn't know when to stop.
- Iteration budgets are suggestions. If a loop hits max iterations without fulfilling its promise, you review what happened and either re-run with adjusted parameters or move on.

## Begin

Start with **Round 1: Project Identity & Existing State**. Ask all Round 1 questions in a single message.

---END---

---

## Reference: Key Concepts

### The Ralph Loop Model

The ralph-loop skill drives iterative execution:

```
/ralph-loop:ralph-loop "prompt with instructions" --max-iterations 15 --completion-promise "PROMISE_NAME"
```

Each invocation:
1. Executes the prompt's instructions step by step
2. Counts iterations (tool calls / reasoning cycles)
3. Looks for `<promise>PROMISE_NAME</promise>` in the output
4. Stops when: promise is detected OR max iterations reached
5. Returns output for user review

The user reviews output between phases, then pastes the next phase's command.

### Concrete vs. Research Fields

Every Input Criteria field accepts one of:

| Mode | Syntax | Effect |
|------|--------|--------|
| **Concrete** | `value: "FastAPI"` | Used directly in all phases. No research needed. |
| **Research** | `research: "What framework best handles...?"` | Phase 1 researches, asks user to confirm, stores as `Decision_` entity in Memory. |
| **Constrained research** | `research: "Compare X vs Y"` with `constraints: [...]` | Narrower research — fewer iterations needed. |

More research fields = more Phase 1 iterations = more tokens but better-informed decisions.

### Execution Modes

| Mode | How it works | When to use |
|------|-------------|-------------|
| **Sequential** | One ralph-loop at a time. User reviews between phases. | Few features, tight dependencies, maximum control |
| **Team** | Phases grouped into waves. Parallel phases spawn teammates. | Many independent features, want throughput |
| **Hybrid** | Mix of sequential and parallel waves. | Some phases depend, others don't |

### Promise Design

Promises signal phase completion. Good promises are:
- **Descriptive** — `MESSAGING_FEATURE_COMPLETE` not `P3_DONE`
- **Unambiguous** — Done criteria must be specific and verifiable
- **One per phase** — Each phase has exactly one promise

### Max Iteration Guidelines

| Phase Type | Suggested Range |
|------------|----------------|
| Bootstrap | 5 |
| Assessment (few research questions) | 12-15 |
| Assessment (many research questions) | 18-22 |
| Foundation (APIs/Auth/Data) | 20-25 |
| Feature implementation | 12-20 |
| Critical path feature | 20-25 |
| Client integration | 15-20 |
| Deployment & hardening | 15-20 |
| Final testing & validation | 20-25 |
| Team lead orchestration | 30-50 |

### Planning Files

The three planning files are the durable state layer:

| File | Purpose | Who writes | Who reads |
|------|---------|-----------|-----------|
| `task_plan.md` | Phase breakdown, status tracking, key decisions | Created in Step 0, updated every phase | Every phase reads first |
| `findings.md` | Research results, gap analysis, architecture notes | Phase 1 (heavy), all phases (append) | All phases |
| `progress.md` | Action log, test results, verification output | Every phase appends | Next phase reads |

These files persist across context windows. If your context gets compressed or you start a new session, re-read these files to restore state.

### How Research Flows Through Phases

```
Input Criteria                    Phase 1 Assessment              Phases 2+
┌─────────────────┐              ┌──────────────────┐            ┌──────────────────┐
│ field_name:      │              │                  │            │                  │
│   research: "?"  │─────────────│  Research tools   │            │ Read Memory:     │
│   constraints:   │   feeds     │  resolve to       │  stores    │ "Decision_       │
│   ["Must be Y"]  │   into      │  concrete choice  │──────────│  field_name"     │
│                  │              │  + rationale      │  Memory   │ → Use resolved   │
└─────────────────┘              └──────────────────┘            └──────────────────┘
```

### Between Phases (Sequential Mode)

After each ralph-loop completes:

1. **Review output** — Check progress.md for what was accomplished
2. **Check findings** — Review findings.md for new discoveries or blockers
3. **Verify decisions** — `mcp__memory__search_nodes('Decision_')` if Memory is available
4. **Verify checkpoint** — `/checkpoint:list` if checkpoint skill is available
5. **Decide next** — Proceed to next phase, or re-run current phase if the promise wasn't fulfilled

### Cancelling a Stalled Loop

```
/ralph-loop:cancel-ralph
```

Then diagnose with progress.md and Memory state before re-running with an adjusted prompt.

---

## What the Interview Produces

After the 6-round interview, Claude Code generates these artifacts:

```
Project Root/
├── RALPH_LOOP_EXECUTION_GUIDE.md     # The populated execution guide (main artifact)
├── task_plan.md                       # Phase breakdown with status tracking
├── findings.md                        # Interview decisions + research questions
└── progress.md                        # Interview completion record
```

### Artifact Relationships

```
┌──────────────────────────────────────────────────────────────────────────┐
│                        INTERVIEW (6 rounds)                              │
│                                                                          │
│  Round 1: Identity ──→ Round 2: Tech ──→ Round 3: Architecture           │
│  Round 4: Features ──→ Round 5: Execution Mode ──→ Round 6: Criteria     │
│                                                                          │
│  Every answer feeds into ──┐                                             │
└────────────────────────────┼─────────────────────────────────────────────┘
                             │
                             ▼
              ┌──────────────────────────────┐
              │  RALPH_LOOP_TEMPLATE.md      │ (read-only template)
              │  + Interview Answers         │
              │  = Populated Execution Guide │
              └──────────────┬───────────────┘
                             │
              ┌──────────────┼──────────────────────┐
              │              │                      │
              ▼              ▼                      ▼
   ┌─────────────────┐  ┌──────────┐  ┌───────────────────────────┐
   │ task_plan.md     │  │findings.md│  │ RALPH_LOOP_EXECUTION_     │
   │ (phases as tasks │  │(decisions │  │ GUIDE.md                  │
   │  pending/done)   │  │ + open Qs)│  │ (the ralph-loop commands  │
   └─────────────────┘  └──────────┘  │  to run each phase)        │
                                       └───────────────────────────┘
```

### How Planning Files Flow Through Execution

```
INTERVIEW PHASE                           EXECUTION PHASE
═══════════════                           ═══════════════

findings.md ◄── Interview decisions       findings.md ◄── Gap analysis, research results,
                                                          architecture notes, promise outcomes

task_plan.md ◄── Phase list (all pending) task_plan.md ◄── Phase status updated per-phase
                                                           (pending → in_progress → complete)

progress.md ◄── "Interview complete,      progress.md ◄── Per-phase action log, test results,
                 guide generated"                          iteration counts, checkpoint records
```

**Important:** Step 0 Bootstrap in the execution guide re-initializes the planning files
for the actual build. The interview-phase planning files serve as input/reference only.

---

## Example: What a Populated Round Looks Like

Here's what Round 5 might look like in practice (this round is unique to the ralph-loop model):

```
Claude Code:  Round 5 is about execution mode and loop configuration.
              Based on your answers so far:
              - 3 features (messaging, voice, file uploads)
              - Voice is on the critical path
              - Messaging and file uploads are independent of each other
              - 2 research fields to resolve in Phase 1

              Let me recommend an execution mode.

[AskUserQuestion]
  Question: "Which execution mode fits this project?"
  Header: "Exec mode"
  Options:
    - "Sequential (Recommended) — 3 features is manageable one-at-a-time,
       and the critical path feature (voice) benefits from your review
       between phases"
    - "Hybrid — Run voice sequentially, then messaging + file uploads
       in parallel as a team wave"
    - "Team — All feature phases run as parallel teammates"
    - (Other)

User: "Hybrid — I want to review voice results before the others start"

Claude Code:  Good choice. For hybrid mode, here's the wave structure
              I'd propose:

              Wave 0 (solo): Step 0 Bootstrap
              Wave 1 (solo): Phase 1 Assessment (resolves 2 research fields)
              Wave 2 (solo): Phase 2 Foundation (APIs, auth, data layer)
              Wave 3 (solo): Phase 3 Voice Communication (critical path —
                             you review before proceeding)
              Wave 4 (parallel): Phase 4 Messaging + Phase 5 File Uploads
              Wave 5 (solo): Phase Y Client Integration
              Wave 6 (solo): Phase D Deployment + Phase F Validation

              Does this wave grouping look right?

User: "Yes, but put deployment and client integration in the same wave"

Claude Code:  Updated. Now for iteration budgets — with 2 research fields,
              Phase 1 needs more room.

[AskUserQuestion]
  Question: "Iteration budget preference?"
  Header: "Budget"
  Options:
    - "Conservative — Lower iterations, may need re-runs if phases
       don't complete"
    - "Generous (Recommended) — Higher iterations, more tokens but
       fewer manual re-runs"
    - "Let me set each phase manually"
    - (Other)

User: "Generous"

Claude Code:  Here's the iteration table I'll use:

              Phase 1 Assessment:    18 iterations (2 research fields)
              Phase 2 Foundation:    22 iterations
              Phase 3 Voice (crit):  25 iterations
              Phase 4 Messaging:     15 iterations
              Phase 5 File Uploads:  12 iterations
              Phase Y Client:        18 iterations
              Phase D Deployment:    18 iterations
              Phase F Validation:    22 iterations

              Promise names will be designed in Round 6.
```

---

## Customizing the Interview

### Adding Rounds

If your project has domain-specific concerns not covered by the 6 default rounds, add them to the kickoff prompt:

```
### Round 7: [Your Domain]
**Goal:** [What this round determines]

Ask about:
- [Question 1]
- [Question 2]
- ...
```

### Skipping Rounds

If you already have a fully specified project (all concrete values, no research needed), you can skip rounds by pre-filling answers in the kickoff prompt:

```
## Pre-filled Answers (skip these questions)

- Project name: "Voice Chat"
- Tech stack: all concrete (FastAPI, Python, React, PostgreSQL, Redis, Docker Compose, Electron)
- Services: auth, user, server, channel, message, file, webrtc, websocket
- Execution mode: sequential (no teams)

## Start the interview at Round 4 (Features)
```

### Deepening Rounds

For complex projects, you can request deeper investigation in specific rounds:

```
## Deep Dive Requests

- Round 2 (Tech Stack): For every research field, I want you to do preliminary
  web research DURING the interview (not just in Phase 1) so I can make a more
  informed choice. Use Brave search + PAL thinkdeep before presenting options.

- Round 5 (Execution Mode): Show me concrete token cost estimates for each
  execution mode based on my feature count and wave structure.

- Round 6 (Promises): For the critical path feature, show me 3 alternative
  promise designs with different granularity levels.
```

---

## Quick Reference: Interview → Template Field Mapping

| Interview Round | Template Section | Key Fields Populated |
|-----------------|-----------------|---------------------|
| Round 1: Identity | PROJECT IDENTITY | project_name, project_slug, prompt_file, goal_summary |
| Round 1: Identity | DEPLOYMENT TARGET | hosting, target_users, geo_distribution |
| Round 2: Tech Stack | ARCHITECTURE | tech_stack.* (each as value or research) |
| Round 3: Architecture | ARCHITECTURE | services, service_base_path, service_entry_file |
| Round 4: Features | FEATURES | features[] (name, category, critical_path, approach) |
| Round 5: Execution Mode | EXECUTION MODE | execution_mode, team_config.* (waves, teammates, delegate_lead, etc.) |
| Round 6: Criteria | SUCCESS CRITERIA | success_criteria.* (functional, infra, deploy, perf) |
| Round 6: Criteria | Phase sections | Promise names and done criteria for each phase |
